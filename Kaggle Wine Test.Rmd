---
title: "Kaggle Red Wine Quality"
author: "Jessica Angier"
date: "January 30, 2019"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Red Wine Quality

The data set is sourced from Kaggle and contains wine data of the Portuguese "Vinho Verde" wine.  The inspiration of this dataset is to use machine learning to determine physichemical properties that would make a good wine.  

www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009/home

## Objective

Use machine learning to determine physichemical properties that would make a good wine.  The dependant variable would be based on the quality of the wine.

## Data Description

The data contains 1,599 observations accross 12 variables.

* Fixed Acidity - most acids involved with wine or fixed or nonvolatile (do not evaporate readily)
* volatile acidity- the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste
* citric acid- found in small quantities, citric acid can add 'freshness' and flavor to wines
* residual sugar- the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet
* chlorides- the amount of salt in the wine
* free sulfur dioxide- the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine
* total sulfur dioxide - amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine
* density - the density of water is close to that of water depending on the percent alcohol and sugar content
* pH- describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale
* sulphates- a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant
* alcohol- the percent alcohol content of the wine
* quality - output variable (based on sensory data, score between 0 and 10)

```{r, message = FALSE, warning=FALSE, echo=FALSE}
#Load libraries and data
library(readr)
library(gridExtra)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrplot)
library(reshape2)
library(dplyr)
library(psych)
library(randomForest)
library(skimr)
wine <- read_csv("data/winequality-red.csv")

#Define chart color with a variable
chartcolor <- "#722F37"
alertcolor <- "#e15759"
trendcolor <- "#2a5783"

#3db6c8
#edc948 - greeny-yellow

#Define chart attributes
mychartattrib <- theme_minimal() +
  theme(plot.subtitle = element_text(size=10, color="#49525e")) +
  theme(plot.caption = element_text(size=9, color="#49525e"))+
  theme(panel.border = 
          element_blank(),
        panel.grid.major = 
          element_blank(), panel.grid.minor = 
          element_blank(),
        axis.line = element_line(color="gray"),
        axis.ticks.x=element_blank(),
        axis.ticks.y=element_blank())
```


```{r}
wine %>% skim() %>% kable()
```

## Exploratory data analysis

### Wine Quality

Wine quality is the target variable of the analysis so it's important we understand its distribution.  There are a lot more normal wines than good or bad ones.  For the analysis we will be classifying the wines into 4 classes: good, normal, tolerable and bad.

* good = quality greater than 7
* normal = qaulity of 6
* tolerable = quality of 5
* bad = under 4

```{r}
#fix column names
colnames(wine) <- wine %>% colnames() %>% str_replace_all(" ","_")

#add new variables
wine <- wine %>%
  mutate(wine_score = ifelse(quality %in% 7:10, "Good", 
                             ifelse(quality %in% 0:4, "Bad", 
                                    ifelse(quality==6, "Normal", "Tolerable"))),
         wine_score = factor(wine_score, levels=c("Bad", "Tolerable", "Normal", "Good")))
```

```{r}
table(wine$wine_score)
```


```{r, fig.height = 3, fig.width = 8}
library(RColorBrewer)
my_cols <- c(brewer.pal(9,"RdGy"))[1:4] # select 4 colors from class Reds

p1 <- ggplot(wine, aes(quality)) +
  geom_bar(stat="count", fill=chartcolor) +
    labs(title = "Wine quality scores") +
    scale_x_continuous(breaks = seq(1,10,1))+
  mychartattrib

p2 <- ggplot(wine, aes(wine_score, fill=wine_score)) +
  geom_bar(stat="count") +
    labs(title = "Wine Classes based on score") +
  scale_fill_manual(values = my_cols) +
  mychartattrib

grid.arrange(
  p1,
  p2,
  nrow=1
)
```

### Correlation

http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram 

Here we are trying to understand the strength of the relationship between the predictors of wine quality.  Positive correlations are displayed in blue where negative correlations are displayed in red.  Color intensity and size are proportional to the correlation coefficients.  From below we are seeing that alcohol and volatile acidity are the most correlated with wine quality, where alcohol is negativly correlated and volatile acidity is positvely correlated.

```{r}
#describe(wine)
w <- cor(wine[1:12])
corrplot(w, type = "upper")
```

### Chemical properties

Here are the distributions of the physichemical properties of wine.  Density and pH both appear the closest to a normal distribution.  

```{r, fig.width=8}
wine %>%
  select(-quality) %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(x=value, fill=key)) +
  geom_histogram(bins=sqrt(nrow(wine))) +
  facet_wrap(~key, scales="free") +
  mychartattrib +
  theme(legend.position = "none") 
```

```{r, fig.width=8}
wine %>%
  select(-quality) %>%
  gather(key, value, -wine_score) %>%
  ggplot(aes(x=value, fill=wine_score)) +
  geom_density(alpha=0.25) +
  facet_wrap(~key, scales="free") +
  scale_fill_manual(values = my_cols) +
  mychartattrib
```

```{r}

```





## Modeling 

The goal is to model the class of the wine based on variables that are the chemical properties of the wine.  Using EDA we have determine that alcohol, volatile acidity, citric acid, and sulphates appear to be the most correlated with wine quality.  

### Random Forest for variable importance 

```{r}
library(randomForest)
wine2 <- wine %>% select(-quality)
fit <- randomForest(wine_score ~., data = wine2, importance=T)
importance(fit) # view results 
varImpPlot(fit,main="Feature Relevance Scores")

```

### Random Forest modeling

Initial unbalanced data set - randomly divide into train and test stratified by class and perform Random Forest with 5x5 repeated cross-validation.

```{r}
#random sample 75/25 split
library(caTools)
set.seed(123)
split = sample.split(wine2$wine_score, SplitRatio = 0.75)
train = subset(wine2, split == TRUE)
test = subset(wine2, split == FALSE)

```

```{r}
# Feature Scaling
train[-12] <- scale(train[-12])
test[-12] <- scale(test[-12])
```

```{r}
# model on train
library(caret)
set.seed(42)
model_rf <- train(wine_score ~ .,
                         data = train,
                         method = "rf",
                         preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 5, 
                                                  repeats = 5, 
                                                  verboseIter = FALSE))
```

```{r}
final <- data.frame(actual = test$wine_score,
                    predict(model_rf, newdata = test))
```

```{r}
cm_original <- confusionMatrix(final$predict, test$wine_score)
cm_original
```

Use SMOTE for unbalanced dataset - "Synthetic Minority Over-sampling Technique" - method that applies over-sampling to the minority class creating synthetic minority class examples.

```{r}
#smote 
ctrl <- trainControl(method = "repeatedcv", 
                     number = 5, 
                     repeats = 5, 
                     verboseIter = FALSE,
                     sampling = "smote")

set.seed(42)
model_rf_smote <- train(wine_score ~ .,
                              data = train,
                              method = "rf",
                              preProcess = c("scale", "center"),
                              trControl = ctrl)
```

```{r}
final_smote <- data.frame(actual = test$wine_score,
                         predict(model_rf_smote, newdata = test))
```

```{r}
cm_smote <- confusionMatrix(final_smote$predict, test$wine_score)
cm_smote
```

Compare predictions for the sampling methods

```{r}
models <- list(original = model_rf,
                       smote = model_rf_smote)
resampling <- resamples(models)
bwplot(resampling)
```


```{r}

# Feature Scaling
#train = scale(training_set[-3])
#test_set[-3] = scale(test_set[-3])


#compare train/test sets
table(train$wine_score)
table(test$wine_score)
```


```{r}
#baseline model
rf_base <-randomForest(wine_score~.,train,ntree=150)

#Confusion matrix results
preds <- predict(rf_base, newdata=test)
table(preds, test$wine_score) #confusion matrix for test set

confusionMatrix(table(preds, test$wine_score))

```

### Decision tree

```{r}
library(rpart)
library(DMwR)

#apply model to training set
#dtr_model <- rpartXse(quality ~., data = train)

#predict values using test
#prediction <- predict(dtr_model,test) 

```

```{r}
#library(rpart.plot) 
#prp(dtr_model,extra=101,box.col="orange",split.box.col="grey")
```




## Sources and references:

* Torgo, Luis. Data Mining in R. Oakville, Canada: Apple Academic Press Inc., 2017.  Print. 

* https://shiring.github.io/machine_learning/2017/04/02/unbalanced

* data source: www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009/home 

* https://www.kaggle.com/meepbobeep/intro-to-regression-and-classification-in-r/code

* https://www.kaggle.com/mrshih/time-to-wine-down-and-avoid-pour-decisions-eda 

* https://www.kaggle.com/grosvenpaul/beginners-guide-to-eda-and-random-forest-using-r 
